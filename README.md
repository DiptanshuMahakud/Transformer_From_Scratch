# Transformer from Scratch

This project is an implementation of the Transformer model from the groundbreaking paper **"Attention Is All You Need"** by Vaswani et al. This project was built entirely from scratch without using any high-level transformer libraries like HuggingFace or PyTorch Transformer APIs.

The goal was to gain a deep understanding of how Transformers work by coding each component from scratch, including:

- **Multi-Head Attention Mechanism**
- **Positional Encoding**
- **Feed Forward Network**
- **Encoder Block**
- **Decoder Block**
- **Add & Norm Layer**
- **Final Linear Layer + Softmax**

The model is capable of performing **Language Modeling tasks** like **Next-Word Prediction**.

---

## ðŸ“œ References
- **[Attention Is All You Need Paper](https://arxiv.org/abs/1706.03762)**.
- CampusX YouTube Series on Transformer Implementation.

I added my own understanding, tweaks, and customization to make the project my own.

---

## ðŸš€ Project Structure

