Transformer from Scratch ðŸš€
This is my implementation of the Transformer model from scratch, inspired by the "Attention is All You Need" paper. I built it using PyTorch, trying to keep everything as close to the paper as possible with a few minor tweaks.

The model consists of:
Embedding Layer for converting tokens to vectors.
Positional Encoding to inject sequence information.
Encoder Block with Multi-Head Attention, Feed Forward, and Add & Norm.
Decoder Block with Masked Attention, Cross Attention, Feed Forward, and Add & Norm.
Transformer combining the Encoder and Decoder.
I plan to use this for a language modeling task (next-word prediction).

Just a fun project to build the Transformer from scratch! ðŸ˜Ž
